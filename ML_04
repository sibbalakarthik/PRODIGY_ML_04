import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import os
import pickle
import time
from collections import deque
import matplotlib.pyplot as plt

class HandGestureRecognizer:
    def __init__(self, model_path=None, confidence_threshold=0.7):
        """
        Initialize the Hand Gesture Recognition system
        
        Args:
            model_path: Path to saved model (optional)
            confidence_threshold: Minimum confidence for gesture prediction
        """
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        self.confidence_threshold = confidence_threshold
        self.label_encoder = LabelEncoder()
        self.model = None
        self.gesture_buffer = deque(maxlen=10)  # For smoothing predictions
        
        # Gesture classes (can be customized)
        self.gesture_classes = [
            'thumbs_up', 'thumbs_down', 'peace', 'fist', 'open_palm',
            'pointing', 'ok_sign', 'rock', 'paper', 'scissors'
        ]
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def extract_landmarks(self, image):
        """
        Extract hand landmarks from image using MediaPipe
        
        Args:
            image: Input image (BGR format)
            
        Returns:
            landmarks: Normalized landmark coordinates or None
        """
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.mp_hands.process(rgb_image)
        
        if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]
            landmarks = []
            
            # Extract x, y coordinates for each landmark
            for landmark in hand_landmarks.landmark:
                landmarks.extend([landmark.x, landmark.y])
            
            return np.array(landmarks), hand_landmarks
        
        return None, None
    
    def preprocess_landmarks(self, landmarks):
        """
        Preprocess landmarks for model input
        
        Args:
            landmarks: Raw landmark coordinates
            
        Returns:
            processed_landmarks: Normalized and processed landmarks
        """
        if landmarks is None:
            return None
        
        # Normalize relative to wrist (landmark 0)
        wrist_x, wrist_y = landmarks[0], landmarks[1]
        normalized_landmarks = landmarks.copy()
        
        for i in range(0, len(landmarks), 2):
            normalized_landmarks[i] -= wrist_x      # x coordinate
            normalized_landmarks[i+1] -= wrist_y    # y coordinate
        
        # Calculate additional features
        features = self.calculate_gesture_features(landmarks)
        
        # Combine landmarks and features
        return np.concatenate([normalized_landmarks, features])
    
    def calculate_gesture_features(self, landmarks):
        """
        Calculate additional geometric features from landmarks
        
        Args:
            landmarks: Hand landmark coordinates
            
        Returns:
            features: Additional geometric features
        """
        features = []
        
        # Convert to (x, y) pairs
        points = [(landmarks[i], landmarks[i+1]) for i in range(0, len(landmarks), 2)]
        
        # Finger tip and pip landmarks
        fingertips = [4, 8, 12, 16, 20]  # Thumb, Index, Middle, Ring, Pinky tips
        finger_pips = [3, 6, 10, 14, 18]  # PIP joints
        
        # Calculate distances from wrist to fingertips
        wrist = points[0]
        for tip_idx in fingertips:
            tip = points[tip_idx]
            distance = np.sqrt((tip[0] - wrist[0])**2 + (tip[1] - wrist[1])**2)
            features.append(distance)
        
        # Calculate angles between fingers
        for i in range(len(fingertips)-1):
            p1 = points[fingertips[i]]
            p2 = points[fingertips[i+1]]
            angle = np.arctan2(p2[1] - wrist[1], p2[0] - wrist[0]) - \
                   np.arctan2(p1[1] - wrist[1], p1[0] - wrist[0])
            features.append(angle)
        
        # Finger extension features (tip vs pip distance from wrist)
        for tip_idx, pip_idx in zip(fingertips[1:], finger_pips[1:]):  # Skip thumb
            tip_dist = np.sqrt((points[tip_idx][0] - wrist[0])**2 + 
                             (points[tip_idx][1] - wrist[1])**2)
            pip_dist = np.sqrt((points[pip_idx][0] - wrist[0])**2 + 
                             (points[pip_idx][1] - wrist[1])**2)
            extension_ratio = tip_dist / (pip_dist + 1e-6)
            features.append(extension_ratio)
        
        return np.array(features)
    
    def create_model(self, input_shape):
        """
        Create neural network model for gesture classification
        
        Args:
            input_shape: Shape of input features
            
        Returns:
            model: Compiled Keras model
        """
        model = keras.Sequential([
            keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(32, activation='relu'),
            keras.layers.Dense(len(self.gesture_classes), activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train_model(self, X_train, y_train, epochs=100, batch_size=32):
        """
        Train the gesture recognition model
        
        Args:
            X_train: Training features
            y_train: Training labels
            epochs: Number of training epochs
            batch_size: Training batch size
        """
        # Encode labels
        y_encoded = self.label_encoder.fit_transform(y_train)
        y_categorical = keras.utils.to_categorical(y_encoded, len(self.gesture_classes))
        
        # Split data
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train, y_categorical, test_size=0.2, random_state=42
        )
        
        # Create model
        self.model = self.create_model(X_train.shape[1])
        
        # Define callbacks
        callbacks = [
            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
        ]
        
        # Train model
        history = self.model.fit(
            X_train_split, y_train_split,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict_gesture(self, landmarks, smooth=True):
        """
        Predict gesture from landmarks
        
        Args:
            landmarks: Hand landmark coordinates
            smooth: Whether to apply temporal smoothing
            
        Returns:
            gesture: Predicted gesture name
            confidence: Prediction confidence
        """
        if self.model is None or landmarks is None:
            return None, 0.0
        
        # Preprocess landmarks
        features = self.preprocess_landmarks(landmarks)
        if features is None:
            return None, 0.0
        
        # Make prediction
        features = features.reshape(1, -1)
        prediction = self.model.predict(features, verbose=0)[0]
        
        if smooth:
            # Add to buffer for smoothing
            self.gesture_buffer.append(prediction)
            
            # Average predictions over buffer
            if len(self.gesture_buffer) > 1:
                prediction = np.mean(list(self.gesture_buffer), axis=0)
        
        # Get predicted class and confidence
        predicted_class = np.argmax(prediction)
        confidence = prediction[predicted_class]
        
        if confidence >= self.confidence_threshold:
            gesture = self.label_encoder.inverse_transform([predicted_class])[0]
            return gesture, confidence
        
        return None, confidence
    
    def process_video_frame(self, frame):
        """
        Process single video frame for gesture recognition
        
        Args:
            frame: Input video frame
            
        Returns:
            frame: Processed frame with annotations
            gesture: Detected gesture
            confidence: Detection confidence
        """
        landmarks, hand_landmarks = self.extract_landmarks(frame)
        gesture, confidence = None, 0.0
        
        if landmarks is not None:
            # Draw hand landmarks
            self.mp_drawing.draw_landmarks(
                frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS
            )
            
            # Predict gesture
            gesture, confidence = self.predict_gesture(landmarks)
        
        # Draw prediction on frame
        if gesture:
            cv2.putText(frame, f'Gesture: {gesture}', (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(frame, f'Confidence: {confidence:.2f}', (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        else:
            cv2.putText(frame, 'No gesture detected', (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        return frame, gesture, confidence
    
    def real_time_recognition(self, camera_index=0):
        """
        Run real-time gesture recognition from webcam
        
        Args:
            camera_index: Camera device index
        """
        cap = cv2.VideoCapture(camera_index)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        print("Starting real-time gesture recognition...")
        print("Press 'q' to quit")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame = cv2.flip(frame, 1)  # Mirror the frame
            processed_frame, gesture, confidence = self.process_video_frame(frame)
            
            cv2.imshow('Hand Gesture Recognition', processed_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
    
    def save_model(self, model_path, encoder_path=None):
        """
        Save trained model and label encoder
        
        Args:
            model_path: Path to save the model
            encoder_path: Path to save the label encoder
        """
        if self.model:
            self.model.save(model_path)
            print(f"Model saved to {model_path}")
        
        if encoder_path is None:
            encoder_path = model_path.replace('.h5', '_encoder.pkl')
        
        with open(encoder_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)
        print(f"Label encoder saved to {encoder_path}")
    
    def load_model(self, model_path, encoder_path=None):
        """
        Load trained model and label encoder
        
        Args:
            model_path: Path to the saved model
            encoder_path: Path to the saved label encoder
        """
        self.model = keras.models.load_model(model_path)
        print(f"Model loaded from {model_path}")
        
        if encoder_path is None:
            encoder_path = model_path.replace('.h5', '_encoder.pkl')
        
        if os.path.exists(encoder_path):
            with open(encoder_path, 'rb') as f:
                self.label_encoder = pickle.load(f)
            print(f"Label encoder loaded from {encoder_path}")

# Data collection utility
class GestureDataCollector:
    def __init__(self, recognizer):
        """
        Initialize data collector
        
        Args:
            recognizer: HandGestureRecognizer instance
        """
        self.recognizer = recognizer
        self.collected_data = []
        self.collected_labels = []
    
    def collect_gesture_data(self, gesture_name, num_samples=100, camera_index=0):
        """
        Collect training data for a specific gesture
        
        Args:
            gesture_name: Name of the gesture to collect
            num_samples: Number of samples to collect
            camera_index: Camera device index
        """
        cap = cv2.VideoCapture(camera_index)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        samples_collected = 0
        print(f"Collecting data for gesture: {gesture_name}")
        print(f"Press SPACE to capture sample, 'q' to quit early")
        print(f"Target: {num_samples} samples")
        
        while samples_collected < num_samples:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame = cv2.flip(frame, 1)
            landmarks, hand_landmarks = self.recognizer.extract_landmarks(frame)
            
            if landmarks is not None:
                # Draw hand landmarks
                self.recognizer.mp_drawing.draw_landmarks(
                    frame, hand_landmarks, self.recognizer.mp_hands.HAND_CONNECTIONS
                )
                
                # Show capture instruction
                cv2.putText(frame, f'Gesture: {gesture_name}', (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, f'Samples: {samples_collected}/{num_samples}', (10, 70),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
                cv2.putText(frame, 'Press SPACE to capture', (10, 110),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            cv2.imshow('Data Collection', frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord(' ') and landmarks is not None:
                # Capture sample
                features = self.recognizer.preprocess_landmarks(landmarks)
                if features is not None:
                    self.collected_data.append(features)
                    self.collected_labels.append(gesture_name)
                    samples_collected += 1
                    print(f"Sample {samples_collected} captured")
            elif key == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        print(f"Collected {samples_collected} samples for {gesture_name}")
    
    def save_dataset(self, filepath):
        """
        Save collected dataset
        
        Args:
            filepath: Path to save the dataset
        """
        dataset = {
            'data': np.array(self.collected_data),
            'labels': np.array(self.collected_labels)
        }
        np.savez(filepath, **dataset)
        print(f"Dataset saved to {filepath}")
    
    def load_dataset(self, filepath):
        """
        Load saved dataset
        
        Args:
            filepath: Path to the saved dataset
        """
        dataset = np.load(filepath)
        self.collected_data = dataset['data'].tolist()
        self.collected_labels = dataset['labels'].tolist()
        print(f"Dataset loaded from {filepath}")

# Example usage and training script
def main():
    # Initialize recognizer
    recognizer = HandGestureRecognizer()
    
    # Option 1: Collect training data
    print("1. Collect training data")
    print("2. Train model")
    print("3. Test real-time recognition")
    choice = input("Choose option (1-3): ")
    
    if choice == '1':
        # Data collection
        collector = GestureDataCollector(recognizer)
        
        gestures_to_collect = ['thumbs_up', 'thumbs_down', 'peace', 'fist', 'open_palm']
        
        for gesture in gestures_to_collect:
            print(f"\nPrepare for {gesture} gesture collection...")
            input("Press Enter when ready...")
            collector.collect_gesture_data(gesture, num_samples=50)
        
        collector.save_dataset('gesture_dataset.npz')
    
    elif choice == '2':
        # Model training
        if not os.path.exists('gesture_dataset.npz'):
            print("Dataset not found. Please collect data first.")
            return
        
        # Load dataset
        dataset = np.load('gesture_dataset.npz')
        X = dataset['data']
        y = dataset['labels']
        
        print(f"Training with {len(X)} samples")
        
        # Train model
        history = recognizer.train_model(X, y, epochs=100)
        
        # Save trained model
        recognizer.save_model('gesture_model.h5')
        
        # Plot training history
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Training Accuracy')
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy')
        plt.legend()
        
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.legend()
        
        plt.tight_layout()
        plt.show()
    
    elif choice == '3':
        # Real-time testing
        if not os.path.exists('gesture_model.h5'):
            print("Trained model not found. Please train the model first.")
            return
        
        # Load trained model
        recognizer.load_model('gesture_model.h5')
        
        # Start real-time recognition
        recognizer.real_time_recognition()

if __name__ == "__main__":
    main()
